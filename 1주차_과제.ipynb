{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(a) :\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0.3, 2.9, 4.0])\n",
    "exp_a = np.exp(a)\n",
    "sum_exp_a = np.sum(exp_a)\n",
    "y = exp_a / sum_exp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 2.9 4. ]\n",
      "[ 1.34985881 18.17414537 54.59815003]\n",
      "74.1221542101633\n",
      "[0.01821127 0.24519181 0.73659691]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(exp_a)\n",
    "print(sum_exp_a)\n",
    "print(y)\n",
    "print(sum(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class로 softmax 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, learning_rate=0.01, threshold=0.01, max_iterations=100000, verbose=False, reg_strength=1e-5):\n",
    "        self._learning_rate = learning_rate  # 학습 계수\n",
    "        self._max_iterations = max_iterations  # 반복 횟수\n",
    "        self._threshold = threshold  # 학습 중단 계수\n",
    "        self._verbose = verbose  # 중간 진행사항 출력 여부\n",
    "        self._reg_strength = reg_strength # 정규화 파라미터 계수\n",
    "\n",
    "    # theta(W) 계수들 return\n",
    "    def get_coeff(self):\n",
    "        return self._W\n",
    "\n",
    "    # softmax function\n",
    "    def softmax_func(self, x_data):\n",
    "        predictions = x_data - (x_data.max(axis=1).reshape([-1, 1]))\n",
    "        softmax = np.exp(predictions)\n",
    "        softmax /= softmax.sum(axis=1).reshape([-1, 1])\n",
    "        return softmax\n",
    "\n",
    "        # prediction result example\n",
    "        # [[0.01821127 0.24519181 0.73659691]\n",
    "        # [0.87279747 0.0791784  0.04802413]\n",
    "        # [0.05280815 0.86841135 0.0787805 ]]\n",
    "\n",
    "    # cost function 정의\n",
    "    def cost_func(self, softmax, y_data):\n",
    "        sample_size = y_data.shape[0]\n",
    "\n",
    "        # softmax[np.arange(len(softmax)), np.argmax(y_data, axis=1)]\n",
    "        # --> 해당 one-hot 의 class index * 해당 유닛의 출력을 각 row(1개의 input row)에 대해 계산\n",
    "        # --> (n, 1) 의 shape\n",
    "        cost = -np.log(softmax[np.arange(len(softmax)), np.argmax(y_data, axis=1)]).sum() \n",
    "        cost /= sample_size\n",
    "        cost += (self._reg_strength * (self._W**2).sum()) / 2\n",
    "        return cost\n",
    "\n",
    "    # gradient 계산 (regularized)\n",
    "    def gradient_func(self, softmax, x_data, y_data):\n",
    "        sample_size = y.shape[0]\n",
    "\n",
    "        # softmax cost function의 미분 결과는 pi−yi 이므로,\n",
    "        # softmax가 계산된 matrix에서, (해당 one-hot 의 class index * 해당 유닛)에 해당하는 유닛 위치에 -1을 더해줌.\n",
    "        softmax[np.arange(len(softmax)), np.argmax(y_data, axis=1)] -= 1\n",
    "        gradient = np.dot(x_data.transpose(), softmax) / sample_size\n",
    "        gradient += self._reg_strength * self._W\n",
    "        return gradient\n",
    "\n",
    "    # learning\n",
    "    def fit(self, x_data, y_data):\n",
    "        num_examples, num_features = np.shape(x_data)\n",
    "        num_classes = y.shape[1]\n",
    "\n",
    "        # 가중계수 초기화\n",
    "        self._W = np.random.randn(num_features, num_classes) / np.sqrt(num_features / 2)\n",
    "\n",
    "        for i in range(self._max_iterations):\n",
    "            \n",
    "            # y^ 계산\n",
    "            z = np.dot(x_data, self._W)\n",
    "            softmax = self.softmax_func(z)\n",
    "\n",
    "            # cost 함수\n",
    "            cost = self.cost_func(softmax, y_data)\n",
    "\n",
    "            # softmax 함수의 gradient (regularized)\n",
    "            gradient = self.gradient_func(softmax, x_data, y_data)\n",
    "\n",
    "            # gradient에 따라 theta 업데이트\n",
    "            self._W -= self._learning_rate * gradient\n",
    "\n",
    "            # 판정 임계값에 다다르면 학습 중단\n",
    "            if cost < self._threshold:\n",
    "                return False\n",
    "\n",
    "            # 100 iter 마다 cost 출력\n",
    "            if (self._verbose == True and i % 100 == 0):\n",
    "                print (\"Iter(Epoch): %s, Loss: %s\" % (i, cost))\n",
    "\n",
    "    # prediction\n",
    "    def predict(self, x_data):\n",
    "        return np.argmax(x_data.dot(self._W), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from sklearn import datasets\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, threshold=0.01, max_iterations=100000, fit_intercept=True, verbose=False):\n",
    "        self._learning_rate = learning_rate  # 학습 계수\n",
    "        self._max_iterations = max_iterations  # 반복 횟수\n",
    "        self._threshold = threshold  # 학습 중단 계수\n",
    "        self._fit_intercept = fit_intercept  # 절편 사용 여부를 결정\n",
    "        self._verbose = verbose  # 중간 진행사항 출력 여부\n",
    "\n",
    "    # theta(W) 계수들 return\n",
    "    def get_coeff(self):\n",
    "        return self._W\n",
    "\n",
    "    # 절편 추가\n",
    "    def add_intercept(self, x_data):\n",
    "        intercept = np.ones((x_data.shape[0], 1))\n",
    "        return np.concatenate((intercept, x_data), axis=1)\n",
    "\n",
    "    # 시그모이드 함수(로지스틱 함수)\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def cost(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "    def fit(self, x_data, y_data):\n",
    "        num_examples, num_features = np.shape(x_data)\n",
    "\n",
    "        if self._fit_intercept:\n",
    "            x_data = self.add_intercept(x_data)\n",
    "\n",
    "        # weights initialization\n",
    "        self._W = np.zeros(x_data.shape[1])\n",
    "\n",
    "        for i in range(self._max_iterations):\n",
    "            z = np.dot(x_data, self._W)\n",
    "            hypothesis = self.sigmoid(z)\n",
    "\n",
    "            # 실제값과 예측값의 차이\n",
    "            diff = hypothesis - y_data\n",
    "\n",
    "            # cost 함수\n",
    "            cost = self.cost(hypothesis, y_data)\n",
    "\n",
    "            # cost 함수의 편미분 : transposed X * diff / n\n",
    "            # 증명 : https://stats.stackexchange.com/questions/278771/how-is-the-cost-function-from-logistic-regression-derivated\n",
    "            gradient = np.dot(x_data.transpose(), diff) / num_examples\n",
    "\n",
    "            # gradient에 따라 theta 업데이트\n",
    "            self._W -= self._learning_rate * gradient\n",
    "\n",
    "            # 판정 임계값에 다다르면 학습 중단\n",
    "            if cost < self._threshold:\n",
    "                return False\n",
    "\n",
    "            # 100 iter 마다 cost 출력\n",
    "            if (self._verbose == True and i % 100 == 0):\n",
    "                print('cost :', cost)\n",
    "\n",
    "    def predict_prob(self, x_data):\n",
    "        if self._fit_intercept:\n",
    "            x_data = self.add_intercept(x_data)\n",
    "\n",
    "        return self.sigmoid(np.dot(x_data, self._W))\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        # 0,1 에 대한 판정 임계값은 0.5 -> round 함수로 반올림\n",
    "        return self.predict_prob(x_data).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
